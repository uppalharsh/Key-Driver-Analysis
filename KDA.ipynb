{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load required libraries\n",
    "\n",
    "#Mathematical, dataframe and directory operations\n",
    "import numpy as np         #--> for numerical and scientific operations\n",
    "import pandas as pd        #--> for dataframe, slicing operations\n",
    "import os                  #--> working directory operations\n",
    "import warnings            #--> Handle warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter  #--> for Count/iteration operations \n",
    "\n",
    "#Visualization libraries\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline   \n",
    "#--> to include visualization in jupyter notebook\n",
    "\n",
    "\n",
    "#Model related libraries\n",
    "from sklearn.linear_model import RidgeCV, LassoCV       #--> For Feature Selection\n",
    "from statsmodels.regression.linear_model import OLS     #--> Linear_Regression Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and adjust working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check current working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change working directory\n",
    "os.chdir('C:\\\\Users\\\\win10\\\\Downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load the data into data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('SampleData(1).xls')  #read_excel will load excel file (.xls in our case) into dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()                                #Shows first 5 rows/records of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() \n",
    "#This line of code will give us information about-\n",
    "# 1. Data type of variables/features present in dataframe so that we can change datatype of certain features as per\n",
    "#    the requirement.\n",
    "# 2. Null Values present in each variable/feature. When this code will be executed, it will tell the number of non-null\n",
    "#    values along with datatype. For example -  'Internet quality      6256 non-null float64'. This means that there are\n",
    "#    6256 non null values out of 10000 values in Internet quality column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "#  This code will give us the summary of our data. It will give count of non null values, mean of the feature/variable,\n",
    "#  standard deviation, minimum value, 25,50,75 percentile of data and maximum value. This information helps us to identify \n",
    "#  the following-\n",
    "#  1. Missing values in a feature/variable by checking 'count' value.\n",
    "#  2. Standard deviation - Higher standard deviation will affect our analysis. Which is why if its higher we need to check\n",
    "#     causes for the same. standard deviation value can be seen in 'std'. \n",
    "#  3. Mininmum and maximum values can be checked using 'min' and 'max'. These values helps to identify if there are outliers\n",
    "#     present in our dataset. Outlier values generally varies on the basis of domain knowledge.\n",
    "#  4. Percentile values of 25, 50, 75 tells about the distribution of data.  \n",
    "\n",
    "\n",
    "#  This function is not applicable for categorical variable since they doesn't have mean score, minimum and maximum values\n",
    "#  etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Value Percentage in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with missing percentage\n",
    "missing_val = pd.DataFrame(df.isnull().sum())\n",
    "\n",
    "#Reset index\n",
    "missing_val = missing_val.reset_index()\n",
    "\n",
    "#Rename variable\n",
    "missing_val = missing_val.rename(columns = {'index': 'Variables', 0: 'Missing_percentage'})\n",
    "\n",
    "#Calculate percentage\n",
    "missing_val['Missing_percentage'] = (missing_val['Missing_percentage']/len(df))*100\n",
    "\n",
    "#descending order\n",
    "missing_val = missing_val.sort_values('Missing_percentage', ascending = False).reset_index(drop = True)\n",
    "\n",
    "#save output results \n",
    "missing_val.to_csv(\"Miising_perc.csv\", index = False)\n",
    "\n",
    "missing_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of rows with missing values in atleast one column\n",
    "df.shape[0] - df.dropna().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the above analysis we can clearly see that our data set contains null values which is most of the part of the dataset. However as per the problem statement we have to perform our analysis with and without null values individually. So we will keep them for now and will see what are the insights that we can get when we have null values in our data set. After that we will impute/remove the null values and see what are the new insights we can get.\n",
    "\n",
    "###### Since we have a limited range of values in our dataset for all features, we will not do outlier analysis in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lets check the distribution of our target variable numerically and visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df['Overall Experience'])\n",
    "#This code will Count the number of occurence of every individual value present in the feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))                   #--> Adjusting figure size\n",
    "sns.countplot(df['Overall Experience'])      #--> Countplot function in seaborn library to visualize distribution of\n",
    "                                             #    target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Here we can clearly see that our target variable that is 'Overall Experience' has more values with good customer satisfaction. Our target variable does not have any missing values which is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART - I     With Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(' ', '_')  # remove whitespace present between column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()      # This code is generally use to see column names present in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We will rearrange the position of our target variable for the ease of analysis. We will place it at the last position of our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Internet_quality', 'Ease_of_making_reservation',\n",
    "       'Attitude_of_hotel_staff', 'Cleanliness_of_room', 'Quietness_of_room',\n",
    "       'Breakfast_quality', 'Cleanliness_of_bathroom', 'Bar_ambiance',\n",
    "       'Accuracy_of_bill', 'Quality_of_business_center',\n",
    "       'Timeliness_of_airport_shuttle', 'Overall_fitness_center',\n",
    "       'Room_service_speed','Overall_Experience']]\n",
    "\n",
    "# We have to place the 'Overall_Experience' column at last when selecting the subset of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(nrows=13,ncols=1, figsize = (20,100)) # Adjust figure size and distribute space for plots. In our case\n",
    "                                                            # 13 rows and 1 column    \n",
    "\n",
    "\n",
    "sns.countplot(df['Internet_quality'], ax=ax[0]).set_title(\"Count Plot Internet_quality\")\n",
    "sns.countplot(df['Ease_of_making_reservation'], ax=ax[1]).set_title(\"Count Plot Ease_of_making_reservation\")\n",
    "sns.countplot(df['Attitude_of_hotel_staff'], ax=ax[2]).set_title(\"Count Plot Attitude_of_hotel_staff\")\n",
    "sns.countplot(df['Cleanliness_of_room'], ax=ax[3]).set_title(\"Count Plot Cleanliness_of_room\")\n",
    "sns.countplot(df['Quietness_of_room'], ax=ax[4]).set_title(\"Count Plot Quietness_of_room\")\n",
    "sns.countplot(df['Breakfast_quality'], ax=ax[5]).set_title(\"Count Plot Breakfast_quality\")\n",
    "sns.countplot(df['Cleanliness_of_bathroom'], ax=ax[6]).set_title(\"Count Plot Cleanliness_of_bathroom\")\n",
    "sns.countplot(df['Bar_ambiance'], ax=ax[7]).set_title(\"Count Plot Bar_ambiance\")\n",
    "sns.countplot(df['Accuracy_of_bill'], ax=ax[8]).set_title(\"Count Plot Accuracy_of_bill\")\n",
    "sns.countplot(df['Quality_of_business_center'], ax=ax[9]).set_title(\"Count Plot Quality_of_business_center\")\n",
    "sns.countplot(df['Timeliness_of_airport_shuttle'], ax=ax[10]).set_title(\"Count Plot Timeliness_of_airport_shuttle\")\n",
    "sns.countplot(df['Overall_fitness_center'], ax=ax[11]).set_title(\"Count Plot Overall_fitness_center\")\n",
    "sns.countplot(df['Room_service_speed'], ax=ax[12]).set_title(\"Count Plot Room_service_speed\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#This code will give multiple visualizations/plot as an output in a single window. We can see that ax part in each line \n",
    "#of code is adjusting the plots for various features in a particular sequence as defined.\n",
    "\n",
    "# Lines of codes can be reduced by using loop statements but we will keep it as simple as possible to understand.\n",
    "\n",
    "# ax part will decide the position of countplot in the region mentioned under subplots (that is 13 rows and 1 column) .\n",
    "# For example ax = ax[0] will ive the countplot 1st position in the figure, ax=ax[1] will give 2nd position in figure \n",
    "# and so on.\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From above plots we can clearly see that data is skewed or not uniformly distributed. In accurate words our data is right skewed which means that if we will train our regression model on this data there are chances that it might gives us biased prediction i.e. good customer experience which is not good. This type of data is often referred as imbalance data. However there is a huge possibility that this skewness might be present because of the missing values present in our dataset. Since half of the values are missing from our dataset, that can be one possible reason for this non-uniformly distributed data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### However if we get the same result (skewness) even after dropping or imputing the null values, we can use sampling techniques to get uniformly distributed data sample out of this data or else we have to find more data from which we can get clear insights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feature Selection is an important process in data analysis which tells us which feature is important for our analysis and which is not. We can use various methods of feature selection but for now we will use correlation methods to find which features are important for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the width and hieght of the plot\n",
    "f, ax = plt.subplots(figsize=(14,10))\n",
    "\n",
    "#Generate correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "#Plot using seaborn library\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=plt.cm.Reds,\n",
    "            square=True, ax=ax,annot=True)\n",
    "\n",
    "\n",
    "# corr() function creates a correlation matrix for each and every variable in the dataframe.\n",
    "\n",
    "# square = True --> it sets the shape of blocks in a correlogram.\n",
    "# annot = True  --> will display correlation strength as well in a correlogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(corr[\"Overall_Experience\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.4]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the above correlogram heatmap we can see the correlation among various features in our dataframe. Obviously features that are highly correlated with each other has to removed beacause they contain same data which will be responsible for the prediction of our target variable. But we also have to check the correlation of features with our target variable. If two highly correlated independent features are also highly correlated with the target variable then we will drop one feature out of two whose correlation strength is less if required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Just to make everything clear, independent variables/features are values which are resposible for or which will contribute in prediction of target variable. Whereas Target variable is a value which had to be predicted or is taken into consideration on basis of which regression has to be performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For example cleanliness_of_bathroom and cleanliness_of_room are highly correlated and have strength of 0.84 which is not good for our model. Therefore we can remove these features. But we can also see that these two features are also correlated to Overall_Experience which is our target variable. In this case if we want to remove one variable out of these two, we will go for the one who has less strength of correlation with target variable i.e. 'cleanliness_of_bathroom'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Also this heatmap will help us to reduce the dimentionality of our dataframe hence reducing complexity of whole model by treating multicollinearity. It tells us which features are important for target variables or which features are highly correlated twith target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For example 'Attitude_of_hotel_staff' and 'Overall_Experience' has correlation strength of 0.74 which is pretty good. Also we can see that 'Bar_ambience' and 'Overall_Experience' has correlation strength of 0.41 which idealy is not good in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Still we will take all these features for now in our analysis and see what happens after regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[1:]].corr()['Overall_Experience'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Above line of code directly gives correlation strength of all features with the target variable ('Overall_Experience'). We can see that Overall_Experience has value of 1 which is because it is highly correlated to itself. Also we can see that 'Attitude_of_hotel_staff' and 'Cleanliness_of_room' are variables which are highly correlated with target variable i.e., 'Overall_Experience'.  \n",
    "\n",
    "###### WHICH IS GOOD BY THE WAY.........!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT NOTE....!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### All the above analysis is done keeping missing values in our dataset. Which means that there is a possibility that we might get different insights from the same analysis if we treat missing values. Also because of missing values in out dataset we cannot change datatype of our features. A regreesion model rejects the dataset with null values. Therefore when building a model for regression or classification, the first step that has to be done is preprocessing which includes missing value treatment, outlier detection etc.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART - II    Without Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing values are values which are not present in a particular record of a particular feature/factor/column in dataset. There are various reasons for occurence of missing values but we will not get into that for now. Missing values can be treated either by dropping the records/rows which has missing values or we can impute missing values by using various methods. In our case we cannot drop missing values from our dataset as it will remove most of the data from our dataset. Since most records of our dataset has missing values in atleat one column (some has missing values for multiple features), dropping records with missing value will reduce the dimentionality of our dataset drastically which will result in low power of analysis because of absence of data. Therefore we will proceed with imputation method in this case.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We will use median method for imputing missing values in our dataframe because the data that we have is skewed and because of which we will not use mean method of imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = df.copy()               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Above line of code will create a copy of current dataframe so that we can compare both the datasets i.e., with and without missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(df.median())      #This code will fill missing values with the median value of the feature/column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()                # Cross checking if the values have been imputed or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()                        # Cross checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Internet_quality'] = df['Internet_quality'].astype(int)\n",
    "df['Ease_of_making_reservation'] = df['Ease_of_making_reservation'].astype(int)\n",
    "df['Attitude_of_hotel_staff'] = df['Attitude_of_hotel_staff'].astype(int)\n",
    "df['Cleanliness_of_room'] = df['Cleanliness_of_room'].astype(int)\n",
    "df['Quietness_of_room'] = df['Quietness_of_room'].astype(int)\n",
    "df['Breakfast_quality'] = df['Breakfast_quality'].astype(int)\n",
    "df['Cleanliness_of_bathroom'] = df['Cleanliness_of_bathroom'].astype(int)\n",
    "df['Bar_ambiance'] = df['Bar_ambiance'].astype(int)\n",
    "df['Accuracy_of_bill'] = df['Accuracy_of_bill'].astype(int)\n",
    "df['Quality_of_business_center'] = df['Quality_of_business_center'].astype(int)\n",
    "df['Overall_fitness_center'] = df['Overall_fitness_center'].astype(int)\n",
    "df['Timeliness_of_airport_shuttle'] = df['Timeliness_of_airport_shuttle'].astype(int)\n",
    "df['Room_service_speed'] = df['Room_service_speed'].astype(int)\n",
    "df['Overall_Experience'] = df['Overall_Experience'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Here we can clearly see that there are no missing values in our data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()                       #Check the summary of dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(nrows=13,ncols=2, figsize = (20,100)) #Adjust figure size and distribute space for plots. In our case\n",
    "                                                            # 13 rows and 1 column    \n",
    "\n",
    "\n",
    "sns.countplot(old_df['Internet_quality'], ax=ax[0,0]).set_title(\"Count Plot M_Internet_quality\")  \n",
    "sns.countplot(df['Internet_quality'], ax=ax[0,1]).set_title(\"Count Plot Internet_quality\")\n",
    "\n",
    "sns.countplot(old_df['Ease_of_making_reservation'], ax=ax[1,0]).set_title(\"Count Plot M_Ease_of_making_reservation\")\n",
    "sns.countplot(df['Ease_of_making_reservation'], ax=ax[1,1]).set_title(\"Count Plot Ease_of_making_reservation\")\n",
    "\n",
    "sns.countplot(old_df['Attitude_of_hotel_staff'], ax=ax[2,0]).set_title(\"Count Plot M_Attitude_of_hotel_staff\")\n",
    "sns.countplot(df['Attitude_of_hotel_staff'], ax=ax[2,1]).set_title(\"Count Plot Attitude_of_hotel_staff\")\n",
    "\n",
    "sns.countplot(old_df['Cleanliness_of_room'], ax=ax[3,0]).set_title(\"Count Plot M_Cleanliness_of_room\")\n",
    "sns.countplot(df['Cleanliness_of_room'], ax=ax[3,1]).set_title(\"Count Plot Cleanliness_of_room\")\n",
    "\n",
    "sns.countplot(old_df['Quietness_of_room'], ax=ax[4,0]).set_title(\"Count Plot M_Quietness_of_room\")\n",
    "sns.countplot(df['Quietness_of_room'], ax=ax[4,1]).set_title(\"Count Plot Quietness_of_room\")\n",
    "\n",
    "sns.countplot(old_df['Breakfast_quality'], ax=ax[5,0]).set_title(\"Count Plot M_Breakfast_quality\")\n",
    "sns.countplot(df['Breakfast_quality'], ax=ax[5,1]).set_title(\"Count Plot Breakfast_quality\")\n",
    "\n",
    "sns.countplot(old_df['Cleanliness_of_bathroom'], ax=ax[6,0]).set_title(\"Count Plot M_Cleanliness_of_bathroom\")\n",
    "sns.countplot(df['Cleanliness_of_bathroom'], ax=ax[6,1]).set_title(\"Count Plot Cleanliness_of_bathroom\")\n",
    "\n",
    "sns.countplot(old_df['Bar_ambiance'], ax=ax[7,0]).set_title(\"Count Plot M_Bar_ambiance\")\n",
    "sns.countplot(df['Bar_ambiance'], ax=ax[7,1]).set_title(\"Count Plot Bar_ambiance\")\n",
    "\n",
    "sns.countplot(old_df['Accuracy_of_bill'], ax=ax[8,0]).set_title(\"Count Plot M_Accuracy_of_bill\")\n",
    "sns.countplot(df['Accuracy_of_bill'], ax=ax[8,1]).set_title(\"Count Plot Accuracy_of_bill\")\n",
    "\n",
    "sns.countplot(old_df['Quality_of_business_center'], ax=ax[9,0]).set_title(\"Count Plot M_Quality_of_business_center\")\n",
    "sns.countplot(df['Quality_of_business_center'], ax=ax[9,1]).set_title(\"Count Plot Quality_of_business_center\")\n",
    "\n",
    "sns.countplot(df['Timeliness_of_airport_shuttle'], ax=ax[10,0]).set_title(\"Count Plot M_Timeliness_of_airport_shuttle\")\n",
    "sns.countplot(df['Timeliness_of_airport_shuttle'], ax=ax[10,1]).set_title(\"Count Plot Timeliness_of_airport_shuttle\")\n",
    "\n",
    "sns.countplot(old_df['Overall_fitness_center'], ax=ax[11,0]).set_title(\"Count Plot M_Overall_fitness_center\")\n",
    "sns.countplot(df['Overall_fitness_center'], ax=ax[11,1]).set_title(\"Count Plot Overall_fitness_center\")\n",
    "\n",
    "sns.countplot(old_df['Room_service_speed'], ax=ax[12,0]).set_title(\"Count Plot M_Room_service_speed\")\n",
    "sns.countplot(df['Room_service_speed'], ax=ax[12,1]).set_title(\"Count Plot Room_service_speed\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#This code will give multiple visualizations/plot as an output in a single window. We can see that ax part in each line \n",
    "#of code is adjusting the plots for various features in a particular sequence as defined.\n",
    "\n",
    "# Lines of codes can be reduced by using loop statements but we will keep it as simple as possible to understand.\n",
    "\n",
    "# We will use our old dataframe (with missing values) and new dataframe (without missing values) to compare visually that\n",
    "# what are the changes that we introduced by imputing missing values in our dataset.\n",
    "\n",
    "# This whole block of code will take some time to load the final figure multiple countplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Here countplots starting with M_ has data from old dataframe (with missing values --> AT LEFT SIDE) and the other one is without missing values (AT RIGHT SIDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From above plots we can clearly see that imputing missing values has some serious effects on few variables. This clearly gives us a better insight. Because of the missing values that were present in our dataset earlier we deduced various insights which clearly was not all correct. We will however make final conclusions at the end of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We will now move to feature selection process for our regression. Here we will implement two feature selection methods. One is Filter method and the other one is wrapper method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter method for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the width and hieght of the plot\n",
    "fig, ax =plt.subplots(nrows=2,ncols=1, figsize = (30,17))\n",
    "\n",
    "#Generate correlation matrix\n",
    "corr = df.corr()\n",
    "Old_corr = old_df.corr()\n",
    "\n",
    "#Plot using seaborn library\n",
    "sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=plt.cm.Reds,\n",
    "            square=True, ax=ax[0],annot=True)\n",
    "\n",
    "sns.heatmap(Old_corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=plt.cm.Reds,\n",
    "            square=True, ax=ax[1],annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For the above figure we have plotted two correlogram for the sake of comparison between with and without missing values datasets. Also we had specifies 2 rows and 1 column for this figure to accomodate both the correlograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(corr[\"Overall_Experience\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Above Block of code will give us features which have correlation strength more that '0.5'. Here '0.5' is our threshold which means that features having correlation coefficient less than 0.5 doesn't contribute much which it comes to decide the customer's overall experience. This value of threshold is not fixed universaly but depends on different models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As we can see that using filter method of feature selection we can literally filter out features which does not have required signoficance for deciding/predicting target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Here we can clearly see that there are four features that are highly correlated with target variable. But if we recall our previous analysis 'Cleanliness_of_room' and 'Cleanliness_of_bathroom' are highly correlated to each other. Therefore we can ignore 'Cleanliness_of_bathroom' in this case since it has low correlation strength with target variable as compared to 'Cleanliness_of_room'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded method of feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Here in embedded method of feature selection we will use Lasso regression. Ridge regression can also be used but we will go with Lasso regression because of its feature selection property. Lasso and Ridge are almost similar to each other. The main difference that we can see betwen both these regularization techniques is that aplha value in Ridge regression takes a good amount of change in it to show the regularized output whereas in Lasso a minor change in alpha value can make a huge difference in magnitudes of coefficients of variables. Just to clarify, ALPHA is the regularization parameter for ridge and lasso regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Overall_Experience')\n",
    "y = df['Overall_Experience']\n",
    "\n",
    "# Here we are subsetting the dataframe into X and y so that we can feed this data into our lasso regression model.\n",
    "# X is the subset of dataframe which had predictors/independent variables whereas y has target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LassoCV()                         # Initializing/Defining a lasso regression model\n",
    "reg.fit(X, y)                           # Fitting the data into model\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)       # Finding out best value of alpha\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))    # Calculating best score based on best alpha value\n",
    "coef = pd.Series(reg.coef_, index = X.columns)                    # Gives output as coefficients of regression results\n",
    "                                                                  # and changing index of output to column/feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()                       # Sorting coefficient values to select important features.\n",
    "plt.figure(figsize=(10,10))                          # Figure size\n",
    "imp_coef.plot(kind = \"barh\")                        # Plotting sorted coefficient values in a 'barh' plot (horizontal)\n",
    "plt.title(\"Feature importance using Lasso Model\")   # Giving title to out plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOILAAAA...!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the above barplot it quite clear which feature are most important when it comes to decide/predict customer's overall experience. The most important feature will be Attitude of hotel staff. But for now we will leave it here and proceed further. We will mention each and every insigth we have encountered throughout this analysis at last. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We will now finally implement our regression model. Note that before implementing our regression model we can work on dimentionality reduction and reduce number of features that has to be fed into our model. But we will skip that part for now and take all our features and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = OLS(df['Overall_Experience'], df[df.columns[0:13]]).fit()\n",
    "\n",
    "linear.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### OLS is ordinary least square method of regression that is available in sklearn (sci-kit learn) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear = LinearRegression()\n",
    "\n",
    "linear.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.Series(linear.coef_, index = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()                       # Sorting coefficient values to select important features.\n",
    "plt.figure(figsize=(10,10))                         # Figure size\n",
    "imp_coef.plot(kind = \"barh\")                        # Plotting sorted coefficient values in a 'barh' plot (horizontal)\n",
    "plt.title(\"Feature importance using regression Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL INSIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Important drivers of guest overall satisfaction for hotel chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From our OLS method we can clearly conclude that 'Attitude_of_hotel_staff' is the most important driver when it comes to overall customer satisfaction. After that 'Cleanliness_of_room' and 'Quiteness_f_room' also contributes to the decision making process of overall customer satisfaction. In our OLS regression analysis we can see that the coefficient values for these three are comparitively greater than every other driver ('Atitude_of_hotel_staff' has coefficient of 0.5037). For the same if we want to visualize it, we have to use LinearRegression function present in sklearn.linear_models library path. This function will help us by giving a list of coefficients as output which we can visualize using barh plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fro the plot again we are getting the exact results which we get during OLS regression analysis. The top three drivers in plot are mostly contributing to decide the overall customer experience. However the most important driver in deciding customer overall satisfaction is 'Attitude_of_hotel_staff'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now the correlation among drivers playes a crucial role here in our model. While doing feature selection, we have seen that there are some variables which are highly correlated to each other.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the correlogram we can see that Cleanliness of bathroom and Cleanliness of room are highly correlated (with strength of 0.84) which actually makes sense because both of them are moslty related to each other. Accuracy of bill and Bar Ambience are also highly correlated with each other ( with strength of 0.84). But when we see the correlation of these highly correlated variables with our target variable, we get to know the difference or impact of correlation among variables. Cleanliness of bathroom has a weaker correlation with target variable as compared to Cleanliness of room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of missing values on regression results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Since we have encountered almost half of the dataset with missing values, we might wanna take into consideration the impact of missing value on our analysis. We can easily do this using visualization techniques. We have done the same for it. Countplots from both the datasets (with and without missing values) are plotted next to each other so that we can see the difference in data distribution. Mostly when data has more than 70-80% of missing value, then that dataset is mostly not usied for crucial decision making instead we wait for more data to come and then we can aggregate all the data, take a sample out of it and then proceed with our analysis. But here we have imputed the missing values with median method (because of the skewness) so that we can perform a regression analysis on given data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The Major impact of missing value will be the wrong prediction or incorrect decision making. For example, lets say Attitude of hotel staff (which is most important driver) has more than 50% of missing values. In this case the final result of our analysis would be totally different. In this case Cleanliness of room will become the most important factor for deciding customer overall satisfaction. Which we all know that it is a biased analysis. That's the only reason preprocessing the data before moving to analysis plays a vital part in any analyis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
